Instalando o kubeADM dentro dos scripts ansible

O ansible como um pipeline instalara os pacotes dentro das instancias
inventariadas no playbook.

MAQUINAS MICRO NAO FUNCIONAM COM KUBERNETES PQ NAO ATINGEM OS REQUISITOS MINIMOS.
2GB RAM OU 2CPU

Iniciamos a instalacao do kubeadm, kubelet e kubectl nas instancias da nuvem 
para podermos gerenciarmos a partir da nossa maquina.

primeiramente criamos uma role para instalar as dependencias:
baseado na documentacao, criamos as pastas:
roles/dependency-packages/tasks/main.yml

para fazer as instalacoes dos recursos podemos usar a funcao builtin do apt module do ansible

apos rodar o script de instalacao, podemos fazer 
aws ssm start-session --target i-xxxxxxx 
para conectarmos as instancias privadas.

---

Antes de instalarmos os pacotes do kubernetes, precisamos fazer o download da public signing key.
A documentacao do kubernetes mostra isso.

---

Agora sera inserido o CRI - Container Runtime Interfaces pq o cluster kubernetes
nao cria containeres. Ele precisa de alguem que o faca e o Kubelet se comunica com
a interface do container para realizar esta operacao.
Usaremos o CRI-O como CRI.
O kubelet gerencia o POD.

---
Instalando o container runtime - CRI-O
dentro das instancias estamos usando o sistema operacional Debian.
Antes, deve-se instalar as dependencias
Durante a instalacao do CRIO(container runtime) tem outro step: 
pode-se encontrar no github crio pq nao esta na documentacao.

No arquivo README, instalation guide, instlations instructions, CRI-O packaging repository, start service

---
E possivel que ocorram erros durante o comando ansible-plabybook...  e seja necessario restartar
o wsl usando o comando wsl --shutdown. Isso pode fazer com que o arquivo /etc/resolv.conf nao tenha
sido reescrito.

conteudo do resolv.conf:
nameserver 0.0.0.0

Assim, ele nao conseguia sair para a internet/AWS.

verificando se o CRI-O esta rodando:
service crio status

---
O cluster autoscaler fara uma interacao com  o auto scaling group da aws, mudando a quantidade de instancias
desejadas. O script userdata sera necessario para atualizar o escalonamento das instancias dentro do cluster autoscaler

---
COMANDOS KUBECTL:

kubectl get nodes
kubectl get po -A -n kube-system

comando para verificar logos dentro do cluster 
kubectl logs -l app=cluster-autoscaler -n kube-system

visualizar eventos:
kubectl get events -n kube-system --sort-by .metadata.creationTimestamp

visualizar pelo config-map (cm)
kubectl get cm -n kube-system
kubectl describe cm -n kube-system cluster-autoscaler-status

criando um scripts de um deploy via linha de comando:
kubectl create deploy nginx --image nginx --replicas 1 --dry-run=client -o yaml

pode ser redirecionado usando o comando acima seguido do "> arquivo.yml"

criando o pod:
kubectl apply -f arquivo.yml (arquivo gerado na linha anterior)

mais comandos:
kubectl get po -o wide

para ver o AUTOSCALER trabalhando
aumentou-se o numero de replicas e a quantidade de instancias nao dava conta, deixando
os pods pendentes.

kubectl scale deploy nginx --replicas 10
kubectl get po

---
Durante a experiencia no autoscaler houve uma ocorreencia de nos nao registrados,
diminuindo o volume das instancias quando nao foi solicitado essa operacao.
Nos nao registrados referem-se ao provider Id. Ate entao, o scale out esta funcionando.

+ comandos:
kubectl edit node ip-xx-xx-x-x.ec2.internal

assim, podemos visualizar o providerID dentro do bloco spec do metadata do pod.

----

No modulo de Cluster Kubernetes, aula 33 - Acesso local e port fowarding, e mostrado
como acessar/operar com os comandos kubectl como se estivesse dentro do cluster kubernetes
nas instancias privadas usando a tecnica de tunelamento, a partir da nossa maquina local.
Comandos para isso abaixo:

De dentro das instancias na raiz, 
cat /etc/kubernetes/admin.conf

copia o conteudo para um arquivo em sua maquina local
sudo vim/nano admin.conf
cola o conteudo do admin.conf

apos isso, executa o port fowarding:
aws ssm start-session --target i-xxxx --document-name AWS-StartPortForwardingSession --parameters '{"portNumber":["6443"], "localPortNumber":["6443"]}'

O comando ainda nao funcionara pq esta apontando para o load balancer e nao consegue ser resolvido
E necessario fazer o seguinte:
exportamos o KUBECONFIG:
export KUBECONFIG=/etc/kubernetes/admin.conf

E edita o /etc/hosts
sudo /etc/hosts
e adiciona a linha abaixo:
127.0.0.1 enderecodoloadbalancer(so o dominio)